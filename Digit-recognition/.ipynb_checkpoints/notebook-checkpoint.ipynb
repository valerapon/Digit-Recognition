{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image, ImageChops\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network_1, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, -1)\n",
    "    \n",
    "class Network_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network_2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
    "        self.drop_out1 = nn.Dropout2d(p=0.25)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "        self.drop_out2 = nn.Dropout2d(p=0.25)\n",
    "        \n",
    "        self.fc1 = nn.Linear(576, 64)\n",
    "        self.drop_out3 = nn.Dropout2d(p=0.25)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.drop_out1(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.drop_out2(x)\n",
    "        \n",
    "        x = x.view(-1, 576)\n",
    "        x = self.fc1(x)\n",
    "        x = self.drop_out3(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, -1)\n",
    "    \n",
    "class Network_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network_3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=7)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5)\n",
    "        self.drop_out1 = nn.Dropout2d(p=0.25)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.drop_out2 = nn.Dropout2d(p=0.25)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.drop_out1(x)\n",
    "        \n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.max_pool2d(x, 3)\n",
    "        x = self.drop_out2(x)\n",
    "        \n",
    "        \n",
    "        print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 3, 3])\n",
      "tensor([[[[0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0014, 0.0000],\n",
      "          [0.0022, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0108, 0.0203, 0.0142],\n",
      "          [0.0162, 0.0228, 0.0140],\n",
      "          [0.0130, 0.0211, 0.0118]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0247, 0.0302, 0.0249],\n",
      "          [0.0304, 0.0218, 0.0282],\n",
      "          [0.0250, 0.0260, 0.0240]],\n",
      "\n",
      "         [[0.0054, 0.0061, 0.0056],\n",
      "          [0.0114, 0.0090, 0.0089],\n",
      "          [0.0035, 0.0060, 0.0034]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000]]]], grad_fn=<ReluBackward0>)\n",
      "tensor([[[[0.0022]],\n",
      "\n",
      "         [[0.0228]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0195]],\n",
      "\n",
      "         [[0.0452]],\n",
      "\n",
      "         [[0.0327]],\n",
      "\n",
      "         [[0.0094]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0273]],\n",
      "\n",
      "         [[0.0478]],\n",
      "\n",
      "         [[0.0449]],\n",
      "\n",
      "         [[0.0368]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0266]],\n",
      "\n",
      "         [[0.0389]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0044]],\n",
      "\n",
      "         [[0.0147]],\n",
      "\n",
      "         [[0.0084]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0421]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0134]],\n",
      "\n",
      "         [[0.0748]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0030]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0303]],\n",
      "\n",
      "         [[0.0205]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0139]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0360]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0147]],\n",
      "\n",
      "         [[0.0267]],\n",
      "\n",
      "         [[0.0521]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0343]],\n",
      "\n",
      "         [[0.0336]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0300]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0100]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0665]],\n",
      "\n",
      "         [[0.0346]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0326]],\n",
      "\n",
      "         [[0.0419]],\n",
      "\n",
      "         [[0.0100]],\n",
      "\n",
      "         [[0.0111]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0064]],\n",
      "\n",
      "         [[0.0137]],\n",
      "\n",
      "         [[0.0253]],\n",
      "\n",
      "         [[0.0598]],\n",
      "\n",
      "         [[0.0115]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0288]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0394]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0413]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0421]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0072]],\n",
      "\n",
      "         [[0.0386]],\n",
      "\n",
      "         [[0.0062]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0072]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0197]],\n",
      "\n",
      "         [[0.0402]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0595]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0339]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0316]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0315]],\n",
      "\n",
      "         [[0.0448]],\n",
      "\n",
      "         [[0.0300]],\n",
      "\n",
      "         [[0.0471]],\n",
      "\n",
      "         [[0.0104]],\n",
      "\n",
      "         [[0.0326]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0194]],\n",
      "\n",
      "         [[0.0245]],\n",
      "\n",
      "         [[0.0103]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0007]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0042]],\n",
      "\n",
      "         [[0.0150]],\n",
      "\n",
      "         [[0.0304]],\n",
      "\n",
      "         [[0.0114]],\n",
      "\n",
      "         [[0.0000]]]], grad_fn=<MaxPool2DWithIndicesBackward>)\n",
      "torch.Size([1, 128, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "e = Network_3()\n",
    "x = torch.rand((1, 1, 28, 28))\n",
    "e.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('data/train.csv')\n",
    "target = torch.tensor(raw_data['label'].to_numpy(), dtype=torch.long)\n",
    "data = torch.tensor(raw_data.iloc[:,1:].to_numpy().reshape((42000, 1, 28, 28)), dtype=torch.long) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomAffine(degrees=(-10, 10), translate=(0, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA = data\n",
    "# TARGET = target\n",
    "\n",
    "# len_data = len(data)\n",
    "# for step in range(5):\n",
    "#     new_data = torch.zeros((len_data, 1, 28, 28))\n",
    "#     for i, tensor in enumerate(data):\n",
    "#         new_data[i] = trans(tensor)\n",
    "#     DATA = torch.cat([DATA, new_data])\n",
    "#     TARGET = torch.cat([TARGET, target])\n",
    "# data = DATA\n",
    "# target = TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(0, len(data))\n",
    "np.random.shuffle(indices)\n",
    "count = int(len(indices) * 0.95)\n",
    "train_idx = indices[:count]\n",
    "test_idx = indices[count:]\n",
    "\n",
    "data = data.cuda()\n",
    "target = target.cuda()\n",
    "\n",
    "X_train, y_train = data[train_idx], target[train_idx]\n",
    "X_test, y_test = data[test_idx], target[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 50\n",
    "batch_size = 10000\n",
    "\n",
    "Net = Network_2().cuda()\n",
    "# optimizer = optim.Adagrad(Net.parameters(), lr=0.01)\n",
    "optimizer = optim.Adam(Net.parameters(), lr=0.01)\n",
    "loss_fun = nn.CrossEntropyLoss().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    Net.train()\n",
    "    for i in range(0, len(y_train), batch_size):\n",
    "        X   = X_train[i:i + batch_size]\n",
    "        y = y_train[i:i + batch_size]\n",
    "\n",
    "        def closure():\n",
    "                optimizer.zero_grad()\n",
    "                output = Net.forward(X)\n",
    "                loss = loss_fun(output, y)\n",
    "                loss.backward()   \n",
    "                if i % 1000 == 0:\n",
    "                        print('Train epoch: %s, {%s}, Loss: %s' % (epoch, i, loss.item()))\n",
    "                return loss\n",
    "        optimizer.step(closure)\n",
    "        \n",
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    Net.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(X_test)):\n",
    "            X = X_test[i:i + 1]\n",
    "            y = y_test[i:i + 1]\n",
    "            \n",
    "            output = Net.forward(X)\n",
    "            pred = output.argmax(dim=1)\n",
    "#             if pred.item() != y.item():\n",
    "#                 print(pred.item(), y.item(), i)\n",
    "            correct += (pred == y).sum().item()\n",
    "        print('TOTAL:', correct / len(X_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 0, {0}, Loss: 2.3030567169189453\n",
      "Train epoch: 0, {10000}, Loss: 2.3674631118774414\n",
      "Train epoch: 0, {20000}, Loss: 2.3165524005889893\n",
      "Train epoch: 0, {30000}, Loss: 2.2775521278381348\n",
      "Train epoch: 1, {0}, Loss: 2.2004168033599854\n",
      "Train epoch: 1, {10000}, Loss: 1.9632834196090698\n",
      "Train epoch: 1, {20000}, Loss: 1.5499621629714966\n",
      "Train epoch: 1, {30000}, Loss: 1.4318822622299194\n",
      "Train epoch: 2, {0}, Loss: 1.3215919733047485\n",
      "Train epoch: 2, {10000}, Loss: 1.1147609949111938\n",
      "Train epoch: 2, {20000}, Loss: 1.1042953729629517\n",
      "Train epoch: 2, {30000}, Loss: 0.9061868786811829\n",
      "Train epoch: 3, {0}, Loss: 0.7739844918251038\n",
      "Train epoch: 3, {10000}, Loss: 0.7262157201766968\n",
      "Train epoch: 3, {20000}, Loss: 0.6580027341842651\n",
      "Train epoch: 3, {30000}, Loss: 0.5697489380836487\n",
      "Train epoch: 4, {0}, Loss: 0.5314523577690125\n",
      "Train epoch: 4, {10000}, Loss: 0.489437997341156\n",
      "Train epoch: 4, {20000}, Loss: 0.46714112162590027\n",
      "Train epoch: 4, {30000}, Loss: 0.4343567490577698\n",
      "Train epoch: 5, {0}, Loss: 0.41417860984802246\n",
      "Train epoch: 5, {10000}, Loss: 0.3719920516014099\n",
      "Train epoch: 5, {20000}, Loss: 0.3585428297519684\n",
      "Train epoch: 5, {30000}, Loss: 0.3058825433254242\n",
      "Train epoch: 6, {0}, Loss: 0.30086207389831543\n",
      "Train epoch: 6, {10000}, Loss: 0.28997567296028137\n",
      "Train epoch: 6, {20000}, Loss: 0.28523722290992737\n",
      "Train epoch: 6, {30000}, Loss: 0.26356565952301025\n",
      "Train epoch: 7, {0}, Loss: 0.2583581507205963\n",
      "Train epoch: 7, {10000}, Loss: 0.24673384428024292\n",
      "Train epoch: 7, {20000}, Loss: 0.2327081859111786\n",
      "Train epoch: 7, {30000}, Loss: 0.21271038055419922\n",
      "Train epoch: 8, {0}, Loss: 0.2154926061630249\n",
      "Train epoch: 8, {10000}, Loss: 0.19861769676208496\n",
      "Train epoch: 8, {20000}, Loss: 0.19534122943878174\n",
      "Train epoch: 8, {30000}, Loss: 0.18726618587970734\n",
      "Train epoch: 9, {0}, Loss: 0.1828119158744812\n",
      "Train epoch: 9, {10000}, Loss: 0.1739574819803238\n",
      "Train epoch: 9, {20000}, Loss: 0.17617753148078918\n",
      "Train epoch: 9, {30000}, Loss: 0.15877750515937805\n",
      "Train epoch: 10, {0}, Loss: 0.1662425845861435\n",
      "Train epoch: 10, {10000}, Loss: 0.15889258682727814\n",
      "Train epoch: 10, {20000}, Loss: 0.15187504887580872\n",
      "Train epoch: 10, {30000}, Loss: 0.1329827606678009\n",
      "Train epoch: 11, {0}, Loss: 0.13708364963531494\n",
      "Train epoch: 11, {10000}, Loss: 0.13843998312950134\n",
      "Train epoch: 11, {20000}, Loss: 0.14020581543445587\n",
      "Train epoch: 11, {30000}, Loss: 0.12021295726299286\n",
      "Train epoch: 12, {0}, Loss: 0.12643882632255554\n",
      "Train epoch: 12, {10000}, Loss: 0.12116876244544983\n",
      "Train epoch: 12, {20000}, Loss: 0.12933166325092316\n",
      "Train epoch: 12, {30000}, Loss: 0.11185690015554428\n",
      "Train epoch: 13, {0}, Loss: 0.1128133162856102\n",
      "Train epoch: 13, {10000}, Loss: 0.11931479722261429\n",
      "Train epoch: 13, {20000}, Loss: 0.12264972180128098\n",
      "Train epoch: 13, {30000}, Loss: 0.1036519706249237\n",
      "Train epoch: 14, {0}, Loss: 0.11001939326524734\n",
      "Train epoch: 14, {10000}, Loss: 0.11148279905319214\n",
      "Train epoch: 14, {20000}, Loss: 0.1107640415430069\n",
      "Train epoch: 14, {30000}, Loss: 0.0948801189661026\n",
      "Train epoch: 15, {0}, Loss: 0.09910470992326736\n",
      "Train epoch: 15, {10000}, Loss: 0.10408790409564972\n",
      "Train epoch: 15, {20000}, Loss: 0.10289548337459564\n",
      "Train epoch: 15, {30000}, Loss: 0.08396077156066895\n",
      "Train epoch: 16, {0}, Loss: 0.09397897124290466\n",
      "Train epoch: 16, {10000}, Loss: 0.09981288760900497\n",
      "Train epoch: 16, {20000}, Loss: 0.09562386572360992\n",
      "Train epoch: 16, {30000}, Loss: 0.07818731665611267\n",
      "Train epoch: 17, {0}, Loss: 0.08758479356765747\n",
      "Train epoch: 17, {10000}, Loss: 0.09032878279685974\n",
      "Train epoch: 17, {20000}, Loss: 0.08973176032304764\n",
      "Train epoch: 17, {30000}, Loss: 0.08147726953029633\n",
      "Train epoch: 18, {0}, Loss: 0.07942540943622589\n",
      "Train epoch: 18, {10000}, Loss: 0.0866696909070015\n",
      "Train epoch: 18, {20000}, Loss: 0.08216588199138641\n",
      "Train epoch: 18, {30000}, Loss: 0.07409525662660599\n",
      "Train epoch: 19, {0}, Loss: 0.07523718476295471\n",
      "Train epoch: 19, {10000}, Loss: 0.08155303448438644\n",
      "Train epoch: 19, {20000}, Loss: 0.08375190943479538\n",
      "Train epoch: 19, {30000}, Loss: 0.07195068895816803\n",
      "Train epoch: 20, {0}, Loss: 0.07347041368484497\n",
      "Train epoch: 20, {10000}, Loss: 0.07798789441585541\n",
      "Train epoch: 20, {20000}, Loss: 0.0801481157541275\n",
      "Train epoch: 20, {30000}, Loss: 0.06697631627321243\n",
      "Train epoch: 21, {0}, Loss: 0.06797289103269577\n",
      "Train epoch: 21, {10000}, Loss: 0.0767529159784317\n",
      "Train epoch: 21, {20000}, Loss: 0.07249423116445541\n",
      "Train epoch: 21, {30000}, Loss: 0.06874888390302658\n",
      "Train epoch: 22, {0}, Loss: 0.06763896346092224\n",
      "Train epoch: 22, {10000}, Loss: 0.06766649335622787\n",
      "Train epoch: 22, {20000}, Loss: 0.06928040832281113\n",
      "Train epoch: 22, {30000}, Loss: 0.06403177231550217\n",
      "Train epoch: 23, {0}, Loss: 0.06319475173950195\n",
      "Train epoch: 23, {10000}, Loss: 0.06756622344255447\n",
      "Train epoch: 23, {20000}, Loss: 0.06758936494588852\n",
      "Train epoch: 23, {30000}, Loss: 0.05722784996032715\n",
      "Train epoch: 24, {0}, Loss: 0.0632154792547226\n",
      "Train epoch: 24, {10000}, Loss: 0.06648509949445724\n",
      "Train epoch: 24, {20000}, Loss: 0.06420882046222687\n",
      "Train epoch: 24, {30000}, Loss: 0.058729346841573715\n",
      "Train epoch: 25, {0}, Loss: 0.056795429438352585\n",
      "Train epoch: 25, {10000}, Loss: 0.06269000470638275\n",
      "Train epoch: 25, {20000}, Loss: 0.06169421970844269\n",
      "Train epoch: 25, {30000}, Loss: 0.053742095828056335\n",
      "Train epoch: 26, {0}, Loss: 0.05750312656164169\n",
      "Train epoch: 26, {10000}, Loss: 0.06192469969391823\n",
      "Train epoch: 26, {20000}, Loss: 0.06355098634958267\n",
      "Train epoch: 26, {30000}, Loss: 0.05772748962044716\n",
      "Train epoch: 27, {0}, Loss: 0.05422463268041611\n",
      "Train epoch: 27, {10000}, Loss: 0.05744999274611473\n",
      "Train epoch: 27, {20000}, Loss: 0.0572868213057518\n",
      "Train epoch: 27, {30000}, Loss: 0.05619809031486511\n",
      "Train epoch: 28, {0}, Loss: 0.05660930275917053\n",
      "Train epoch: 28, {10000}, Loss: 0.055970482528209686\n",
      "Train epoch: 28, {20000}, Loss: 0.053314581513404846\n",
      "Train epoch: 28, {30000}, Loss: 0.052497606724500656\n",
      "Train epoch: 29, {0}, Loss: 0.050668321549892426\n",
      "Train epoch: 29, {10000}, Loss: 0.05794589966535568\n",
      "Train epoch: 29, {20000}, Loss: 0.05739244818687439\n",
      "Train epoch: 29, {30000}, Loss: 0.04885883629322052\n",
      "Train epoch: 30, {0}, Loss: 0.04940539598464966\n",
      "Train epoch: 30, {10000}, Loss: 0.05369146168231964\n",
      "Train epoch: 30, {20000}, Loss: 0.05084611847996712\n",
      "Train epoch: 30, {30000}, Loss: 0.051484253257513046\n",
      "Train epoch: 31, {0}, Loss: 0.04613863676786423\n",
      "Train epoch: 31, {10000}, Loss: 0.050934117287397385\n",
      "Train epoch: 31, {20000}, Loss: 0.05121460556983948\n",
      "Train epoch: 31, {30000}, Loss: 0.04759278520941734\n",
      "Train epoch: 32, {0}, Loss: 0.04826908931136131\n",
      "Train epoch: 32, {10000}, Loss: 0.049827612936496735\n",
      "Train epoch: 32, {20000}, Loss: 0.05282195284962654\n",
      "Train epoch: 32, {30000}, Loss: 0.043334413319826126\n",
      "Train epoch: 33, {0}, Loss: 0.04538271203637123\n",
      "Train epoch: 33, {10000}, Loss: 0.05075691640377045\n",
      "Train epoch: 33, {20000}, Loss: 0.04803725332021713\n",
      "Train epoch: 33, {30000}, Loss: 0.04569735378026962\n",
      "Train epoch: 34, {0}, Loss: 0.04520188271999359\n",
      "Train epoch: 34, {10000}, Loss: 0.05079380050301552\n",
      "Train epoch: 34, {20000}, Loss: 0.04690704494714737\n",
      "Train epoch: 34, {30000}, Loss: 0.04211035370826721\n",
      "Train epoch: 35, {0}, Loss: 0.04219746217131615\n",
      "Train epoch: 35, {10000}, Loss: 0.048235274851322174\n",
      "Train epoch: 35, {20000}, Loss: 0.0463416613638401\n",
      "Train epoch: 35, {30000}, Loss: 0.03932855650782585\n",
      "Train epoch: 36, {0}, Loss: 0.043717287480831146\n",
      "Train epoch: 36, {10000}, Loss: 0.043180473148822784\n",
      "Train epoch: 36, {20000}, Loss: 0.04136098921298981\n",
      "Train epoch: 36, {30000}, Loss: 0.04271897301077843\n",
      "Train epoch: 37, {0}, Loss: 0.04023109748959541\n",
      "Train epoch: 37, {10000}, Loss: 0.043222565203905106\n",
      "Train epoch: 37, {20000}, Loss: 0.04963548108935356\n",
      "Train epoch: 37, {30000}, Loss: 0.04229332506656647\n",
      "Train epoch: 38, {0}, Loss: 0.040086470544338226\n",
      "Train epoch: 38, {10000}, Loss: 0.04704929515719414\n",
      "Train epoch: 38, {20000}, Loss: 0.04195467755198479\n",
      "Train epoch: 38, {30000}, Loss: 0.03966175392270088\n",
      "Train epoch: 39, {0}, Loss: 0.04080400615930557\n",
      "Train epoch: 39, {10000}, Loss: 0.03985913097858429\n",
      "Train epoch: 39, {20000}, Loss: 0.039974749088287354\n",
      "Train epoch: 39, {30000}, Loss: 0.04066334664821625\n",
      "Train epoch: 40, {0}, Loss: 0.03742024675011635\n",
      "Train epoch: 40, {10000}, Loss: 0.04343279078602791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch: 40, {20000}, Loss: 0.038135260343551636\n",
      "Train epoch: 40, {30000}, Loss: 0.033817701041698456\n",
      "Train epoch: 41, {0}, Loss: 0.03537054732441902\n",
      "Train epoch: 41, {10000}, Loss: 0.03749409690499306\n",
      "Train epoch: 41, {20000}, Loss: 0.03653930500149727\n",
      "Train epoch: 41, {30000}, Loss: 0.035875529050827026\n",
      "Train epoch: 42, {0}, Loss: 0.03934619575738907\n",
      "Train epoch: 42, {10000}, Loss: 0.04080572724342346\n",
      "Train epoch: 42, {20000}, Loss: 0.038200899958610535\n",
      "Train epoch: 42, {30000}, Loss: 0.0363578274846077\n",
      "Train epoch: 43, {0}, Loss: 0.032998740673065186\n",
      "Train epoch: 43, {10000}, Loss: 0.04023153334856033\n",
      "Train epoch: 43, {20000}, Loss: 0.036639295518398285\n",
      "Train epoch: 43, {30000}, Loss: 0.0343499556183815\n",
      "Train epoch: 44, {0}, Loss: 0.02941957674920559\n",
      "Train epoch: 44, {10000}, Loss: 0.03509429469704628\n",
      "Train epoch: 44, {20000}, Loss: 0.0387876033782959\n",
      "Train epoch: 44, {30000}, Loss: 0.033823639154434204\n",
      "Train epoch: 45, {0}, Loss: 0.03192853555083275\n",
      "Train epoch: 45, {10000}, Loss: 0.03927086293697357\n",
      "Train epoch: 45, {20000}, Loss: 0.03823358565568924\n",
      "Train epoch: 45, {30000}, Loss: 0.03418503329157829\n",
      "Train epoch: 46, {0}, Loss: 0.03716263547539711\n",
      "Train epoch: 46, {10000}, Loss: 0.03457636386156082\n",
      "Train epoch: 46, {20000}, Loss: 0.03413459286093712\n",
      "Train epoch: 46, {30000}, Loss: 0.030664730817079544\n",
      "Train epoch: 47, {0}, Loss: 0.031337324529886246\n",
      "Train epoch: 47, {10000}, Loss: 0.0340055413544178\n",
      "Train epoch: 47, {20000}, Loss: 0.032495833933353424\n",
      "Train epoch: 47, {30000}, Loss: 0.030930135399103165\n",
      "Train epoch: 48, {0}, Loss: 0.03635355457663536\n",
      "Train epoch: 48, {10000}, Loss: 0.034615084528923035\n",
      "Train epoch: 48, {20000}, Loss: 0.03496512398123741\n",
      "Train epoch: 48, {30000}, Loss: 0.03373732045292854\n",
      "Train epoch: 49, {0}, Loss: 0.030323946848511696\n",
      "Train epoch: 49, {10000}, Loss: 0.03563917800784111\n",
      "Train epoch: 49, {20000}, Loss: 0.03285158798098564\n",
      "Train epoch: 49, {30000}, Loss: 0.029420262202620506\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epoch):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL: 0.9866666666666667\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9823809523809524"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9823809523809524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9745238095238096"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9745238095238096"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
